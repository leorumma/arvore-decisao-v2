{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d209c204",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/leorumma/arvore-decisao-v2/blob/main/arvore-decisao-parte2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344f771",
   "metadata": {
    "id": "a344f771"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from seaborn import heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#criar os gráficos das árvores de decisão\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO\n",
    "from IPython.display import Image  \n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9b5c2-0e25-424d-b1a1-505abbb050b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_decision_tree_png(decision_tree_clf, feature_names, file_name = 'arvore1.png'):\n",
    "    # tem que usar feature_names = one_hot_data.columns pois feature_names = feature_cols tem menos atributos\n",
    "    # pois o one-hot acrescenta mais\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(decision_tree_clf, out_file=dot_data,  \n",
    "                    filled=True, rounded=True,\n",
    "                    special_characters=True,feature_names = feature_names, class_names=['No','Yes'])\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    graph.write_png(file_name)\n",
    "    #clf_image_png = graph.create_png()\n",
    "    \n",
    "    #Image(clf_image_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413230d5-5606-46f0-94b2-f976ea0b6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#link para o dataset: \n",
    "DATASET_URL = 'Online_Payments_Fraud_Detection_Dataset.csv'\n",
    "dataset = pd.read_csv(DATASET_URL)\n",
    "original_dataset = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b67e06-240f-45a0-8a24-9c7703d5cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ac4b1-a021-4e64-8b0e-1592a9eefa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iniciando análise exploratória dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315cc0a-7030-43cb-8a10-9d99080c6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sem valores não preenchidos nas colunas\n",
    "print(len(dataset))\n",
    "print(len(dataset.dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75cc066-f169-44b5-b111-ab485f331948",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501f737-a76a-4fe7-874f-d07e8ecda2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tipos de pagamento/transação\n",
    "type_unique_values = dataset['type'].unique()\n",
    "print(type_unique_values)\n",
    "print(len(type_unique_values))\n",
    "\n",
    "#Conclusão: podemos(e faz sentido) categorizar esses dados e utilizarmos para treinar nosso classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ae3c2-4da8-4947-93d7-23d5c2aa7f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origens dos pagamentos\n",
    "nameOrig_unique_values = dataset['nameOrig'].unique()\n",
    "print(nameOrig_unique_values)\n",
    "print(len(nameOrig_unique_values))\n",
    "\n",
    "#Conclusão: poderíamos utilizar o get_dummies aqui, mas isso faria por adicionar 6353307 colunas\n",
    "# ao nosso dataset, o que é inviável(acho eu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064fa46-6627-494e-a622-c87f59469158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# origens dos pagamentos\n",
    "nameDest_unique_values = dataset['nameDest'].unique()\n",
    "print(nameDest_unique_values)\n",
    "print(len(nameDest_unique_values))\n",
    "\n",
    "#Conclusão: o mesmo acima vale para o atributo nameDest, vai ser dificil utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d05175-9220-450a-890c-7b8667fcebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot do oldbalanceOrg\n",
    "print('| Máximo | Mediana | Mínimo | : | {0} | {1} | {2} |'\\\n",
    "      .format(dataset['oldbalanceOrg'].max(), dataset['oldbalanceOrg'].median(), dataset['oldbalanceOrg'].min()))\n",
    "bplots = plt.boxplot(dataset['oldbalanceOrg'],  vert = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c5dfb-8318-47b3-8c33-6fcb6b2553f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot do newbalanceOrig\n",
    "print('| Máximo | Mediana | Mínimo | : | {0} | {1} | {2} |'\\\n",
    "      .format(dataset['newbalanceOrig'].max(), dataset['newbalanceOrig'].median(), dataset['newbalanceOrig'].min()))\n",
    "bplots = plt.boxplot(dataset['newbalanceOrig'],  vert = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720b888-46cc-4366-acc5-f05414f0df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot do oldbalanceDest\n",
    "print('| Máximo | Mediana | Mínimo | : | {0} | {1} | {2} |'\\\n",
    "      .format(dataset['oldbalanceDest'].max(), dataset['oldbalanceDest'].median(), dataset['oldbalanceDest'].min()))\n",
    "bplots = plt.boxplot(dataset['oldbalanceDest'],  vert = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b6f32-d931-4c23-a3e5-f5392fd00e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot do newbalanceDest\n",
    "print('| Máximo | Mediana | Mínimo | : | {0} | {1} | {2} |'\\\n",
    "      .format(dataset['newbalanceDest'].max(), dataset['newbalanceDest'].median(), dataset['newbalanceDest'].min()))\n",
    "bplots = plt.boxplot(dataset['newbalanceDest'],  vert = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e68ce-dde9-4df8-8f84-4366c053faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusão a partir dos boxplots dos balanços: posso reduzir o número de colunas, \n",
    "# considerar somente o valor que entrou na conta de destino e saiu da conta de origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbce257e-68d5-4194-8218-d8f153d3ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot da coluna step\n",
    "print('| Máximo | Mediana | Mínimo | : | {0} | {1} | {2} |'\\\n",
    "      .format(dataset['step'].max(), dataset['step'].median(), dataset['step'].min()))\n",
    "bplots = plt.boxplot(dataset['step'],  vert = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f9bc18-8f76-4274-9686-31f216119692",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(dataset.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49e023-c91e-41b0-9298-9ddd16c8e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformação do dataset, seleção das features\n",
    "\n",
    "dataset['balanceDeltaOrig'] = original_dataset['newbalanceOrig'] - original_dataset['oldbalanceOrg']\n",
    "dataset['balanceDeltaDest'] = original_dataset['newbalanceDest'] - original_dataset['oldbalanceDest']\n",
    "\n",
    "feature_cols = ['amount','oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "target_col = 'isFraud'\n",
    "\n",
    "dummy_columns = ['type']\n",
    "\n",
    "prepared_dummy_columns = pd.get_dummies(dataset[dummy_columns])\n",
    "\n",
    "combined = pd.concat([dataset, prepared_dummy_columns], axis=1)\n",
    "\n",
    "feature_cols.extend(prepared_dummy_columns.columns.values.tolist())\n",
    "\n",
    "X = combined[feature_cols]\n",
    "y = combined[target_col]\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af257dd3-bf82-4d21-a0d7-0a2615587514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treino\n",
    "test_percentage = 0.05\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=test_percentage)\n",
    "\n",
    "# Criação do classificador de árvore de decisão \n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "# Usamos o método fit para construir o classificador a partir do nosso conjunto de treinamento\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = clf.score(X_train, y_train)\n",
    "\n",
    "# Usando modelo para classificar os dados que temos a disposição\n",
    "y_pred = clf.predict(X_test)\n",
    "test_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('| Train accuracy | Test accuracy | : | {0} | {1} |'.format(train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe195f-5cb8-426c-a141-7aad9e553d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_decision_tree(clf, X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c33299-d4e2-4eb2-9988-2e5d0da44bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teste\n",
    "random_element = random.randint(0, len(X))\n",
    "sample = X.iloc[[random_element]]\n",
    "target_of_sample = y.iloc[[random_element]]\n",
    "\n",
    "pred_sample = clf.predict(sample)\n",
    "\n",
    "print(pred_sample)\n",
    "print(target_of_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd3b30-b6ed-46cb-84e6-74e5fed2c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_test_decision_tree_clf(X, y,criterion, test_percentage, max_depth = 10000):\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=test_percentage)\n",
    "\n",
    "    # Criação do classificador de árvore de decisão \n",
    "    classifier = DecisionTreeClassifier(criterion=criterion, max_depth = max_depth)\n",
    "\n",
    "    # Usamos o método fit para construir o classificador a partir do nosso conjunto de treinamento\n",
    "    classifier = classifier.fit(X_train, y_train)\n",
    "    \n",
    "    train_accuracy = classifier.score(X_train, y_train)\n",
    "\n",
    "    # Usando modelo para classificar os dados que temos a disposição\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return classifier, train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23af9e",
   "metadata": {},
   "source": [
    "# Experimento n:\n",
    "**Situação Inicial:**\n",
    "* Classificador **Entropy**\n",
    "* K-fold 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb8c8341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      " Treino: [      0       1       2 ... 6362617 6362618 6362619] Teste: [     17      23      39 ... 6362536 6362600 6362614]\n",
      "Iteração: 1 \n",
      "Acurácia para o treinamento: 1.0 \n",
      "Acurácia para o teste: 0.999786565978617\n",
      " Treino: [      0       1       2 ... 6362617 6362618 6362619] Teste: [     15      20      57 ... 6362547 6362562 6362565]\n",
      "Iteração: 2 \n",
      "Acurácia para o treinamento: 1.0 \n",
      "Acurácia para o teste: 0.9997557610683143\n",
      " Treino: [      0       1       3 ... 6362617 6362618 6362619] Teste: [      2      40      50 ... 6362559 6362571 6362588]\n",
      "Iteração: 3 \n",
      "Acurácia para o treinamento: 1.0 \n",
      "Acurácia para o teste: 0.999786565978617\n",
      " Treino: [      0       1       2 ... 6362616 6362618 6362619] Teste: [      7       9      34 ... 6362586 6362611 6362617]\n",
      "Iteração: 4 \n",
      "Acurácia para o treinamento: 1.0 \n",
      "Acurácia para o teste: 0.9997843656278811\n",
      " Treino: [      0       1       2 ... 6362615 6362616 6362617] Teste: [      3       4      19 ... 6362607 6362618 6362619]\n",
      "Iteração: 5 \n",
      "Acurácia para o treinamento: 1.0 \n",
      "Acurácia para o teste: 0.9998085694859761\n",
      " Treino: [      0       1       2 ... 6362617 6362618 6362619] Teste: [     51      55      56 ... 6362578 6362581 6362587]\n",
      "Iteração: 6 \n",
      "Acurácia para o treinamento: 1.0 \n",
      "Acurácia para o teste: 0.9997953673815606\n",
      " Treino: [      0       1       2 ... 6362617 6362618 6362619] Teste: [     11      25      28 ... 6362595 6362597 6362613]\n",
      "Iteração: 7 \n",
      "Acurácia para o treinamento: 1.0 \n",
      "Acurácia para o teste: 0.9997711635234656\n",
      " Treino: [      0       1       2 ... 6362617 6362618 6362619] Teste: [     14      22      26 ... 6362566 6362582 6362608]\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=14, shuffle=True, random_state=80)\n",
    "model_dtc_kfold_3_entropy = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "print(kfold.get_n_splits(X))\n",
    "\n",
    "# Médias\n",
    "accuracy_med_train = 0\n",
    "accuracy_med_test = 0\n",
    "count_split = 0\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    count_split +=1\n",
    "    print(\" Treino:\", train_index, \"Teste:\", test_index)\n",
    "    X_train, X_test = X.loc[train_index,:], X.loc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model_dtc_kfold_3_entropy.fit(X_train, y_train)\n",
    "    y_pred_train = model_dtc_kfold_3_entropy.predict(X_train)\n",
    "    y_pred_test = model_dtc_kfold_3_entropy.predict(X_test)\n",
    "    # Cálculo das acurácias\n",
    "    accuracy_train = metrics.accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = metrics.accuracy_score(y_test, y_pred_test)\n",
    "    print(\"Iteração: \" + str(count_split) + \" \\n\" + \"Acurácia para o treinamento: \" + str(accuracy_train) + \" \\n\" + \"Acurácia para o teste: \" + str(accuracy_test))\n",
    "    # Cálculo das médias\n",
    "    accuracy_med_train += accuracy_train/kfold.get_n_splits(X)\n",
    "    accuracy_med_test += accuracy_test/kfold.get_n_splits(X)\n",
    "\n",
    "print(\"Acurácia média para o treinamento:\",accuracy_med_train)\n",
    "print(\"Acurácia média para o teste:\",accuracy_med_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "arvore-decisao-parte2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f196f7554b24332a19ef2d1e38c36eeb19a86dc137a0ce6d9673ab017d6f1b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
